# -*- coding: utf-8 -*-
"""Pratica3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hnUGVDugQg9U8U-rmG72o8gDirhIG2fE
"""

from os import listdir
from google.colab import drive
drive.mount('/content/drive')

"""Carregando dataset"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import glob                                                                                     
import statistics
import json, codecs



#Tentei várias jeitos de puxar esses arquivos direto do Edge impulse, mas não deu certo, entao extrai Json e adicionei na pasta do drive

#treino
n=200
ntest=200
#desligado
dl = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Desligada.2mmlan73.ingestion-db7ccf66d-qgwrs.json')
DesligadoTrain = dl.iat[7,2]
DesligadoTrain=[DesligadoTrain[i:i + n] for i in range(0, len(DesligadoTrain), n)]
DesligadoTrain.pop(len(DesligadoTrain)-1) # aqui estou deletando o ultimo elemento pois ele pode conter poucos dados

#Leve
lv = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Leve.2mmqmu74.ingestion-db7ccf66d-dj8h4.json')
lv1 = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Leve.2mmq4oe5.ingestion-db7ccf66d-hft8p.json')
lv2 = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Leve.2mmq2ij5.ingestion-db7ccf66d-slpts.json')
lv = lv.iat[7,2]
lv1 = lv1.iat[7,2]
lv2 = lv2.iat[7,2]
LeveTrain = lv+lv1+lv2
LeveTrain=[LeveTrain[i:i + n] for i in range(0, len(LeveTrain), n)]
LeveTrain.pop(len(LeveTrain)-1)

#Pesado
df = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Pesado.2mmpdc37.ingestion-db7ccf66d-slpts.json')
df1 = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Pesado.2mmpbadf.ingestion-db7ccf66d-bvt4l.json')
df2 = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/training/Pesado.2mmp98f7.ingestion-db7ccf66d-slpts.json')
df = df.iat[7,2]
df1 = df1.iat[7,2]
df2 = df2.iat[7,2]
PesadoTrain = df+df1+df
PesadoTrain=[PesadoTrain[i:i + n] for i in range(0, len(PesadoTrain), n)]
PesadoTrain.pop(len(PesadoTrain)-1)

#Teste
#Desligado
dl = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/testing/Desligado.2mmpikb7.ingestion-db7ccf66d-bvt4l.json')
DesligadoTest= dl.iat[7,2]
DesligadoTest=[DesligadoTest[i:i + ntest] for i in range(0, len(DesligadoTest), ntest)]
DesligadoTest.pop(len(DesligadoTest)-1)
#Leve 
lv = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/testing/Leve.2mmq6lah.ingestion-db7ccf66d-dj8h5.json')
lv1 = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/testing/Leve.2mmq65gq.ingestion-db7ccf66d-hft8p.json')
lv2 = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/testing/Leve.2mmqnluc.ingestion-db7ccf66d-slpt1.json')
lv = lv.iat[7,2]
lv1 = lv1.iat[7,2]
lv2 = lv2.iat[7,2]
LeveTest = lv+lv1+lv2
LeveTest=[LeveTest[i:i + ntest] for i in range(0, len(LeveTest), ntest)]
LeveTest.pop(len(LeveTest)-1)
#Pesado
df = pd.read_json (r'/content/drive/MyDrive/Colab Notebooks/cmi/testing/Pesado.2mmpem9h.ingestion-db7ccf66d-dj8h5.json')
PesadoTest = df.iat[7,2]
PesadoTest=[PesadoTest[i:i + ntest] for i in range(0, len(PesadoTest), ntest)]
PesadoTest.pop(len(PesadoTest)-1)

"""Plotando => dados de treino"""

def plot_data(df_list, title, y_lim, qtdd ):
  fig, axs = plt.subplots(1, qtdd, figsize=(30,5))
  fig.suptitle(title, fontsize=16)
  for index in range(qtdd):
    axs[index].plot(x, df_list[index] )
    axs[index].set_xlabel('Time', fontsize=14)
    axs[index].set_ylabel('ac', fontsize=14)
    axs[index].set_ylim(y_lim)

x = np.linspace(0, 10, n)
plot_data(DesligadoTrain, 'Desligado', (-4, 12),10)
plot_data(LeveTrain, 'Leve', (-4, 12),10)
plot_data(PesadoTrain, 'Pesado', (-4, 12),10)

"""

Plotando => dados de teste"""

plot_data(DesligadoTest, 'Desligado', (-4, 12),4)
plot_data(LeveTest, 'Leve', (-4, 12),4)
plot_data(PesadoTest, 'Pesado', (-4, 12),4)

#Aqui foi feito o dataset com a soma dos valores estatisticos
def createDataset (data, Value):
    auxValues1 = list()
    auxValues2 = list()
    auxValues3 = list()
    for f in data:
        Column0 = f[:][0]
        Column1 = f[:][1]
        Column2 = f[:][2]
        auxValues1.append([statistics.mean(Column0),max(Column0),min(Column0),statistics.stdev(Column0)])
        auxValues2.append([statistics.mean(Column1),max(Column1),min(Column1),statistics.stdev(Column1)])
        auxValues3.append([statistics.mean(Column2),max(Column2),min(Column2),statistics.stdev(Column2), Value])

    npAuxValues1 = np.array(auxValues1) 
    npAuxValues2 = np.array(auxValues2) 
    npAuxValues3 = np.array(auxValues3) 
    return   np.concatenate((npAuxValues1, npAuxValues2, npAuxValues3), axis=1)




sumstatisticsDesligadoTrain = createDataset(DesligadoTrain, Value = 0)
sumstatisticsLeveTrain = createDataset(LeveTrain, Value = 1)
sumstatisticsPesadoTrain = createDataset(PesadoTrain, Value = 2)

sumstatisticsDesligadoTest = createDataset(DesligadoTest, Value = 0)
sumstatisticsLeveTest = createDataset(LeveTest, Value = 1)
sumstatisticsPesadoTest = createDataset(PesadoTest, Value = 2)

totalDataset = np.append(sumstatisticsDesligadoTrain,sumstatisticsLeveTrain,axis=0)
totalDataset = np.append(totalDataset ,sumstatisticsPesadoTrain,axis=0)
totalTestDataset = np.append(sumstatisticsDesligadoTest,sumstatisticsLeveTest,axis=0)
totalTestDataset = np.append(totalTestDataset ,sumstatisticsPesadoTest,axis=0)
totalTestDataset

from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics


clf = DecisionTreeClassifier(criterion="entropy", max_depth=4)

X_train = totalDataset[:,[0,1,5,7]]
X_test = totalTestDataset[:,[0,1,5,7]]
y_train = totalDataset[:,12]
y_test = totalTestDataset[:,12]


X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))


clf = DecisionTreeClassifier()
clf = clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""Treino usando Randon Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV




param_grid = {  'n_estimators': [1, 2, 4, 6, 8, 10, 25, 50, 100],
                'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
                'criterion': ['entropy']}

rfc = RandomForestClassifier()
gs_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, verbose=False, cv=5, n_jobs=-1, return_train_score=True, scoring='accuracy')
gs_rfc.fit(X_train, y_train)

print(gs_rfc.best_params_)
print(gs_rfc.best_score_)

"""Treino usando perceptron"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

from sklearn.linear_model import Perceptron

ppn = Perceptron(max_iter=1000, eta0=0.01, random_state=1)
ppn.fit(X_train_std, y_train)

y_pred = ppn.predict(X_test_std)
print('Misclassified samples: %d' % (y_test != y_pred).sum())

from sklearn.metrics import accuracy_score

print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))
